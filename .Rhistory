# Final Plot
plot(
graphNetwork,
layout = layout.fruchterman.reingold,
# Force Directed Layout
main = paste(coocTerm, ' Graph'),
vertex.label.family = "sans",
vertex.label.cex = 0.8,
vertex.shape = "circle",
vertex.label.dist = 0.5,
# Labels of the nodes moved slightly
vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
vertex.label.color = 'black',
# Color of node names
vertex.label.font = 2,
# Font of node names
vertex.label = V(graphNetwork)$name,
# node names
vertex.label.cex = 1 # font size of node names
)
require("NLP")
text <- read.csv(paste(save_dir,'text_extracts.csv', sep = "/"), header = T, sep = ";", encoding = "UTF-8")
#text to corpus
sotu_corpus <- corpus(text$text,
docnames = text$doc_id)
doc<- as.String(sotu_corpus)
sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- NLP::annotate(doc, sent_token_annotator)
a1
as.matrix(Reports_char[a1])
as.matrix(doc[a1])
grepl.sub(data = df,
pattern = c("women", "woman"),
Var = "doc[a1]")
grepl.sub(data = df,
pattern = c("women", "woman"),
Var = "doc[a1]")
a1
return(a1)
a1[1:5;]
a1[1:5,]
a1[1:5]
as.matrix(doc[a1])
df <- as.data.frame(df)
as.matrix(doc[a1])
doc <- as.data.frame(as.matrix(doc))
#re-read the csv into data frame
text <- read.csv(paste(save_dir,'text_extracts.csv', sep = "/"), header = T, sep = ";", encoding = "UTF-8")
#text to corpus
sotu_corpus <- corpus(text$text,
docnames = text$doc_id)
# Corpus to string [NLP deals with string]
doc<- as.String(sotu_corpus)
# Annotating sentences
sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- NLP::annotate(doc, sent_token_annotator)
a1[1:5]
# converting into dataframe
as.matrix(doc[a1])
df <- as.data.frame(as.matrix(doc))
View(df)
grepl.sub(data = df,
pattern = c("women", "woman"),
Var = "doc[a1]")
library(DataCombine)
install.packages("DataCombine")
library(DataCombine)
grepl.sub(data = df,
pattern = c("women", "woman"),
Var = "doc[a1]")
#clearing memory & setting up the markdown
rm(list = ls())
# Global options
options(stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE,
fig.height = 6,
fig.width = 8,
message = FALSE,
warning = FALSE
)
#Calling packages (Java 64x bit was installed separately) -- imp to do the same if encountered error "error: JAVA_HOME cannot be determined from the Registry"
library(pdftools)
library(tm)
library(SnowballC)
library(kernlab)
library(dplyr)
library(tidyr)
library(wordcloud)
library(openNLP)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(ngram)
library(stringr)
library(ggplot2)
library(reshape2)
library(gplots)
library(corrplot)
library(here)
library(quanteda)
library(readtext)
library(quanteda.textstats)
library(igraph)
library(DataCombine)
#########################################
# Initializing and setting up File paths #
#########################################
#The project_directory will be the location of where R project is located
#The data_directory will be the location of where input/raw data is stored
#The save_directory will be the location of where outputs are stored
project_dir <- here()
data_dir <- here('01_Data/letters')
resources_dir <- here('01_Data/resources')
save_dir <- here('03_Outputs/')
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc[a1]")
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc[a1]")
a2<- as.matrix(doc[a1])
#clearing memory & setting up the markdown
rm(list = ls())
# Global options
options(stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE,
fig.height = 6,
fig.width = 8,
message = FALSE,
warning = FALSE
)
#Calling packages (Java 64x bit was installed separately) -- imp to do the same if encountered error "error: JAVA_HOME cannot be determined from the Registry"
library(pdftools)
library(tm)
library(SnowballC)
library(kernlab)
library(dplyr)
library(tidyr)
library(wordcloud)
library(openNLP)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(ngram)
library(stringr)
library(ggplot2)
library(reshape2)
library(gplots)
library(corrplot)
library(here)
library(quanteda)
library(readtext)
library(quanteda.textstats)
library(igraph)
library(DataCombine)
#########################################
# Initializing and setting up File paths #
#########################################
#The project_directory will be the location of where R project is located
#The data_directory will be the location of where input/raw data is stored
#The save_directory will be the location of where outputs are stored
project_dir <- here()
data_dir <- here('01_Data/letters')
resources_dir <- here('01_Data/resources')
save_dir <- here('03_Outputs/')
#Importing letters as pdf files
Reports <- list.files(data_dir, pattern = "pdf", full.names=T)
#Extracting texts from the pdfs & checking the length (how many document and pages)
Reports_text <- lapply(Reports, pdf_text)
length(Reports_text)
lapply(Reports_text, length)
# Adding texts to a virtual corpus for further manipulation [you can adjust this, for the sake of demo here, we leave unadjusted]
corp <- VCorpus(VectorSource(Reports_text))
lapply(corp, length) #double checking the length
#Adding meta data on the corpus level and document level
corp
inspect(corp)
#Custom function to add space. will be used below to replace/strip dashes for connected words e.g. "read-friendly"
addspace <- content_transformer(function(x, pattern) {
return(gsub(pattern, " ", x))
})
corp <- tm_map(corp, addspace, "-")
#Custom function to remove special characters e.g "$"
rmchr <- content_transformer(function(x, pattern) {
return(gsub(pattern, " ", x))
})
corp <- tm_map(corp, rmchr, "[[:punct:]]")
#Remove unnecessary elements from text [Note: respect the order]
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removePunctuation, ucp = TRUE)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removeWords, stopwords("english"))
corp <- tm_map(corp, stripWhitespace) #Note: tm package doesn't treat spaces as characters,it's purely cosmetic transformation.
#stemming transformation was commented out as not required for this specific task
#corp <- tm_map(corp, stemDocument)
#checking if the transformation happened
writeLines(head(strwrap(corp[[1]]), 50))
# Term-document Matrix - setting up a matrix of words
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)
#For the sake of processing time and interest of this research, we are not going to count frequencies for each word in the document, but interested in looking into certain key words related to women and wealth management
# A victor of interested words to filter on - Skip to tdm if you are not interested in filtering
My_words <-
c(
"woman",
"women",
"female",
"females",
"wealth",
"entrepreneur",
"business",
"management",
"wealth management",
"hnwi",
"net worth",
"high net",
"Worth individuals"
)
#TDM with bigram and specific words filtered
Reports.tdm <-
TermDocumentMatrix(corp,
control = list(tokenize = BigramTokenizer,
dictionary = My_words))
#inspecting the results
inspect(Reports.tdm)
#opinions.tdm <- removeSparseTerms(opinions.tdm, 0.95)
# reducing dimentionality by removving sparse
# Transform into matrix/dataframe to be able to work with the numbers
as.matrix(Reports.tdm)
v <- sort(rowSums(as.matrix(Reports.tdm)), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
#Visualtizing frequencies
wordcloud(
words = d$word,
freq = d$freq,
random.order = FALSE,
rot.per = 0.35,
scale = c(7, .7),
min.freq  = 1,
max.words = 100,
colors = brewer.pal(8, "Dark2")
)
#setting parapmeters for the axis's margins
par(mar = c(10, 6, 4, 3) + .1)
barplot(
d[1:11,]$freq,
las = 2,
names.arg = d[1:11,]$word,
col = "lightblue",
main = "Most frequent words & corresponding frequencies",
ylab = "Word frequencies"
)
#creating another data frame that counts frequency for each document & visualize it
TF.df<- data.frame(as.table(as.matrix(Reports.tdm)))
TF.df$Docs <- factor(
TF.df$Docs,
levels = c(1, 2),
labels = c("Goldman",
"JPM")
)
ggplot(data = TF.df, aes(x = Terms, y = Freq)) + geom_bar(stat = "identity", aes(fill =
Terms)) +
facet_wrap(Docs ~ .) +
theme(
axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank()
) +
ggtitle("Frequencies per document") +
theme(plot.title = element_text(hjust = 0.5))
# here we will follow a different text processing pipeline to implement the method discussed by: Andreas Niekler, Gregor Wiedemann -- The sequence and logic of processing remains constant across prvious and this method, however, the syntax will differ slightly.
#re-importing the corpus without any transformation as we need it for sentence detection
extracted_texts <- readtext(Reports, docvarsfrom = "filepaths", dvsep = "/")
#save as csv for othe uses if needed
write.csv2(extracted_texts, paste(save_dir,'text_extracts.csv', sep = "/"), fileEncoding = "UTF-8")
#re-read the csv into data frame
text <- read.csv(paste(save_dir,'text_extracts.csv', sep = "/"), header = T, sep = ";", encoding = "UTF-8")
#text to corpus
sotu_corpus <- corpus(text$text,
docnames = text$doc_id)
#corpus to sentences
corpus_sentences <- corpus_reshape(sotu_corpus, to = "sentences")
#applying transformation
# Build a dictionary of lemmas
lemma_data <- read.csv(paste(resources_dir,'baseform_en.tsv', sep = "/"), encoding = "UTF-8")
# read an extended stop word list
stopwords_extended <- readLines(paste(resources_dir,'stopwords_en.txt', sep = "/"), encoding = "UTF-8")
# Preprocessing of the corpus of sentences
corpus_tokens <- corpus_sentences %>%
tokens(
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
) %>%
tokens_tolower() %>%
tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
tokens_remove(pattern = stopwords_extended, padding = T)
#create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. In addition, we are interested in words that the do occur -- note that the interest is not how frequent they occur but that they occur together.
minimumFrequency <- 10
# Create DTM, prune vocabulary and set binary values for presence/absence of types
binDTM <- corpus_tokens %>%
tokens_remove("") %>%
dfm() %>%
dfm_trim(min_docfreq = minimumFrequency, max_docfreq = 1000000) %>%
dfm_weight("boolean")
# Matrix multiplication for co-occurrence counts
coocCounts <- t(binDTM) %*% binDTM
as.matrix(coocCounts[202:205, 202:205])
# Read in the source code for the co-occurrence calculation to calculate Statistical significance -- in order to not only count joint occurrence we have to determine their significance this because only frequently co-occurring could result in poor indicator of meaning.
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 10
# Determination of the term of which co-competitors are to be measured.
coocTerm <- "woman"
coocs <- calculateCoocStatistics(coocTerm, binDTM, measure="LOGLIK")
# Display the numberOfCoocs main terms
print(coocs[1:numberOfCoocs])
#The network graph would help with acquiring an extended semantic environment, by providing a secondary co -occurrence with the terms that co - occurred with the term "woman"
#For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with from, to, sig.
resultGraph <- data.frame(from = character(),
to = character(),
sig = numeric(0))
#First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.
# The structure of the temporary graph object is equal to that of the resultGraph
tmpGraph <- data.frame(from = character(),
to = character(),
sig = numeric(0))
# Fill the data.frame to produce the correct number of lines
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
tmpGraph[, 1] <- coocTerm
# Entry of the co-occurrences into the second column of the respective line
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
# Set the significances
tmpGraph[, 3] <- coocs[1:numberOfCoocs]
# Attach the triples to resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)
# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs) {
# Calling up the co-occurrence calculation for term i from the search words co-occurrences
newCoocTerm <- names(coocs)[i]
coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure = "LOGLIK")
#print the co-occurrences
coocs2[1:10]
# Structure of the temporary graph object
tmpGraph <- data.frame(from = character(),
to = character(),
sig = numeric(0))
tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
tmpGraph[, 1] <- newCoocTerm
tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
#Append the result to the result graph
resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}
#Graphing
# set seed for graph plot
set.seed(1)
# Create the graph object as undirected graph
graphNetwork <- graph.data.frame(resultGraph, directed = F)
# Identification of all nodes with less than 2 edges [co-occurring terms connected to at least 2 terms - filtering out terms cooccuring to only to one term.]
verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 2]
# These edges are removed from the graph
graphNetwork <- delete.vertices(graphNetwork, verticesToRemove)
# Assign colors to nodes (search term blue, others orange)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange')
# Set edge colors
E(graphNetwork)$color <- adjustcolor("DarkGray", alpha.f = .5)
# scale significance between 1 and 10 for edge width
E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))
# Set edges with radius
E(graphNetwork)$curved <- 0.15
# Size the nodes by their degree of networking (scaled between 5 and 15)
V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))
# Define the frame and spacing for the plot
par(mai = c(0, 0, 1, 0))
# Final Plot
plot(
graphNetwork,
layout = layout.fruchterman.reingold,
# Force Directed Layout
main = paste(coocTerm, ' Graph'),
vertex.label.family = "sans",
vertex.label.cex = 0.8,
vertex.shape = "circle",
vertex.label.dist = 0.5,
# Labels of the nodes moved slightly
vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
vertex.label.color = 'black',
# Color of node names
vertex.label.font = 2,
# Font of node names
vertex.label = V(graphNetwork)$name,
# node names
vertex.label.cex = 1 # font size of node names
)
text <- read.csv(paste(save_dir,'text_extracts.csv', sep = "/"), header = T, sep = ";", encoding = "UTF-8")
#text to corpus
sotu_corpus <- corpus(text$text,
docnames = text$doc_id)
# Corpus to string [NLP deals with string]
doc<- as.String(sotu_corpus)
# Annotating sentences
sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- NLP::annotate(doc, sent_token_annotator)
a1[1:5]
# converting into dataframe
as.matrix(doc[a1])
df <- as.data.frame(as.matrix(doc))
grepl.sub(data = df,
pattern = c("women", "woman"),
Var = "doc[a1]")
a2<- as.matrix(doc[a1])
View(a2)
View(a2)
grepl.sub(data = df, pattern = c("women", "woman"),Var = "a2[v1]")
grepl.sub(data = df, pattern = c("women", "woman"),Var = a2&v1)
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc[a1]")
doc[a1]
grepl.sub(data = df, pattern = c("women", "woman"),Var = doc[a1])
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc")
View(df)
View(df)
df <- as.data.frame(a2)
grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
View(terms)
View(terms)
subset <-
subset(df, str_detect(
df$chr,
pattern = c("woman", "women"),
negate = FALSE
))
View(subset)
View(subset)
View(df)
View(df)
subset <-
subset(df, str_detect(
df$V1,
pattern = c("woman", "women"),
negate = FALSE
))
terms
df <- as.data.frame(as.matrix(doc[a1]))
#sub-setting for a pattern
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
write.csv2(terms_in_texts, paste(save_dir,'terms_in_extracts.csv', sep = "/"), fileEncoding = "UTF-8")
write.csv2(terms, paste(save_dir,'terms_in_extracts.csv', sep = "/"), fileEncoding = "UTF-8")
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
View(terms)
View(terms)
write.table(terms, paste(save_dir,'terms_in_extracts.xls', sep = "/"), fileEncoding = "UTF-8")
terms<- grepl.sub(data = doc, pattern = c("women", "woman"),Var = "doc[a1]")
grepl.sub(data = doc, pattern = c("women", "woman"),Var = "doc[a1]")
grepl.sub(data = doc[a1], pattern = c("women", "woman"),Var = "doc[a1]")
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc[a1]")
grepl.sub(data = df, pattern = c("women", "woman"),Var = "doc")
write.csv(terms, paste(save_dir,'terms_in_extracts.xls', sep = "/"), fileEncoding = "UTF-8", row.names = FALSE)
write.csv(terms, paste(save_dir,'terms_in_extracts.xls', sep = "/"), fileEncoding = "UTF-8", row.names = FALSE)
df$V1 <- gsub('\\s+', ' ', df$V1)
df$V1 <- gsub('\\s+', '', df$V1)
df2$V1 <- gsub('\\s+', '', df$V1)
df2 <- gsub('\\s+', '', df$V1)
df2 <- as.data.frame(gsub('\\s+', '', df$V1))
View(df2)
View(df2)
terms_export <- as.data.frame(gsub('\\s+', ' ', df$V1))
View(terms_export)
View(terms_export)
terms_export <- as.data.frame(gsub('\\s+', ' ', terms$V1))
View(terms_export)
View(terms_export)
write.csv(terms, paste(save_dir,'terms_in_extracts2.xlsx', sep = "/"), fileEncoding = "UTF-8", row.names = FALSE)
write.csv(terms, paste(save_dir,'terms_in_extracts2.xls', sep = "/"), fileEncoding = "UTF-8", row.names = FALSE)
write.csv(terms, paste(save_dir,'terms_in_extracts2.csv', sep = "/"), fileEncoding = "UTF-8", row.names = FALSE)
write.csv(terms, paste(save_dir,'terms_in_extracts2.csv'), fileEncoding = "UTF-8", row.names = FALSE)
terms_export <- as.data.frame(gsub("[^[:alnum:] ]", terms_export$V1))
terms_export <- as.data.frame(gsub("[^[:alnum:] ]", "", terms_export$V1))
View(terms_export)
View(terms_export)
terms<- grepl.sub(data = doc, pattern = c("women", "woman"),Var = "doc[a1]")
terms<- grepl.sub(data = doc, pattern = c("women", "woman"),Var = "doc[a1]")
#re-read the csv into data frame
text <- read.csv(paste(save_dir,'text_extracts.csv', sep = "/"), header = T, sep = ";", encoding = "UTF-8")
#text to corpus
sotu_corpus <- corpus(text$text,
docnames = text$doc_id)
# Corpus to string [NLP deals with string]
doc<- as.String(sotu_corpus)
# Annotating sentences
sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- NLP::annotate(doc, sent_token_annotator)
a1[1:5]
# converting into dataframe
df <- as.data.frame(as.matrix(doc[a1]))
#sub-setting for a pattern, parsing the text & extracting in csv file
terms<- grepl.sub(data = doc, pattern = c("women", "woman"),Var = "doc[a1]")
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
terms_export <- as.data.frame(gsub('\\s+', ' ', terms$V1))
terms_export$V1 <- as.data.frame(gsub("[^[:alnum:] ]", "", terms_export$V1))
terms_export2 <- as.data.frame(gsub("[^[:alnum:] ]", "", terms_export$V1))
write.text(terms_export, paste(save_dir,'terms_in_extracts2.csv'), fileEncoding = "UTF-8", row.names = FALSE)
write.csv(terms_export, paste(save_dir,'terms_in_extracts2.csv'), fileEncoding = "UTF-8", row.names = FALSE)
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
terms_export <- as.data.frame(gsub([^[:alnum:] ], ' ', terms$V1))
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")
terms_export <- as.data.frame(gsub("[^[:alnum:] ]", ' ', terms$V1))
