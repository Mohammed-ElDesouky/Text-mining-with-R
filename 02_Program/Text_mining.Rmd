---
title: "Text Mining"
author: "Mohammed ElDesouky"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
#clearing memory & setting up the markdown
rm(list = ls())

# Global options
options(stringsAsFactors = FALSE)


knitr::opts_chunk$set(echo = TRUE,
	fig.height = 6,
	fig.width = 8,
	message = FALSE,
	warning = FALSE
	)


#Calling packages (Java 64x bit was installed separately) -- imp to do the same if encountered error "error: JAVA_HOME cannot be determined from the Registry"
library(pdftools)
library(tm)
library(SnowballC)
library(kernlab)
library(dplyr)
library(tidyr)
library(wordcloud)
library(wordcloud2)
library(openNLP)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
library(ngram)
library(stringr)
library(ggplot2)
library(reshape2)
library(gplots)
library(corrplot)
library(here)
library(quanteda)
library(readtext)
library(quanteda.textstats)
library(igraph)
library(DataCombine)
library(topicmodels)


#########################################
# Initializing and setting up File paths #
#########################################
#The project_directory will be the location of where R project is located
#The data_directory will be the location of where input/raw data is stored
#The save_directory will be the location of where outputs are stored
project_dir <- here()
data_dir <- here('01_Data/letters')
resources_dir <- here('01_Data/resources')
save_dir <- here('03_Outputs/')


```

```{r letters pre-processing}
#Importing letters as pdf files
Reports <- list.files(data_dir, pattern = "pdf", full.names=T)

#Extracting texts from the pdfs & checking the length (how many document and pages)
Reports_text <- lapply(Reports, pdf_text)
length(Reports_text)
lapply(Reports_text, length)

# Adding texts to a virtual corpus for further manipulation [you can adjust this, for the sake of demo here, we leave unadjusted]
corp <- VCorpus(VectorSource(Reports_text))
lapply(corp, length) #double checking the length

#Adding meta data on the corpus level and document level
corp

inspect(corp)

```
```{r data transformation}
#Custom function to add space. will be used below to replace/strip dashes for connected words e.g. "read-friendly"
addspace <- content_transformer(function(x, pattern) {
  return(gsub(pattern, " ", x))
})                            

corp <- tm_map(corp, addspace, "-")

#Custom function to remove special characters e.g "$"
rmchr <- content_transformer(function(x, pattern) {
  return(gsub(pattern, " ", x))
})                            

corp <- tm_map(corp, rmchr, "[[:punct:]]")

#Remove unnecessary elements from text [Note: respect the order]
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removePunctuation, ucp = TRUE)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removeWords, stopwords("english"))
corp <- tm_map(corp, stripWhitespace) #Note: tm package doesn't treat spaces as characters,it's purely cosmetic transformation.

#stemming transformation was commented out as not required for this specific task
#corp <- tm_map(corp, stemDocument)   

#checking if the transformation happened
writeLines(head(strwrap(corp[[1]]), 50))  

# Term-document Matrix - setting up a matrix of words
BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)

```
``` {r data frequent words}
#For the sake of processing time and interest of this research, we are not going to count frequencies for each word in the document, but interested in looking into certain key words related to women and wealth management
# A victor of interested words to filter on - Skip to tdm if you are not interested in filtering
My_words <-
  c(
    "women",
    "female",
    "wealth",
    "entrepreneur",
    "business",
    "management",
    "wealth management",
    "net worth"
  )

#TDM with bigram and specific words filtered
Reports.tdm <-
  TermDocumentMatrix(corp,
                     control = list(tokenize = BigramTokenizer,
                                    dictionary = My_words))

#inspecting the results
inspect(Reports.tdm)

#opinions.tdm <- removeSparseTerms(opinions.tdm, 0.95)
# reducing dimentionality by removving sparse


# Transform into matrix/dataframe to be able to work with the numbers
as.matrix(Reports.tdm)
v <- sort(rowSums(as.matrix(Reports.tdm)), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)

#Visualtizing frequencies
wordcloud(
  words = d$word,
  freq = d$freq,
  random.order = FALSE,
  rot.per = 0.35,
  scale = c(7, .7),
  min.freq  = 1,
  max.words = 100,
  colors = brewer.pal(8, "Dark2")
)
  #setting parapmeters for the axis's margins
par(mar = c(10, 6, 4, 3) + .1)  
barplot(
  d[1:11,]$freq,
  las = 2,
  names.arg = d[1:11,]$word,
  col = "lightblue",
  main = "Most frequent words & corresponding frequencies",
  ylab = "Word frequencies"
)
```

```{r frequency per document}
#creating another data frame that counts frequency for each document & visualize it
TF.df<- data.frame(as.table(as.matrix(Reports.tdm)))
  
  TF.df$Docs <- factor(
    TF.df$Docs,
    levels = c(1, 2, 3, 4, 5, 6),
    labels = c("BanksofAmerica",
               "Citigroup",
               "Goldmansachs",
               "JPM",
               "MS",
               "Prudential")
    )
  
  ggplot(data = TF.df, aes(x = Terms, y = Freq)) + geom_bar(stat = "identity", aes(fill =
                                                                                     Terms)) +
    facet_wrap(Docs ~ .) +
    theme(
      axis.title.x = element_blank(),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank()
    ) +
    ggtitle("Frequencies per document") +
    theme(plot.title = element_text(hjust = 0.5))
  

```

```{r topic models: identification using ML model}
# here we will follow a different text processing pipeline to implement the method discussed by: Andreas Niekler, Gregor Wiedemann -- The sequence and logic of processing remains constant across previous and this method, however, the syntax will differ slightly. source: https://tm4ss.github.io/docs/Tutorial_6_Topic_Models.html

#re-importing the corpus without any transformation as we need it for sentence detection
extracted_texts <- readtext(Reports, docvarsfrom = "filepaths", dvsep = "/")

#save as csv for othe uses if needed
write.csv2(extracted_texts, paste(save_dir,'text_extracts.csv', sep = "/"), fileEncoding = "UTF-8")

#text to corpus
sotu_corpus <- corpus(extracted_texts$text,
                      docnames = extracted_texts$doc_id)

#corpus to sentences
corpus_sentences <- corpus_reshape(sotu_corpus, to = "sentences")

#applying transformation

# Build a dictionary of lemmas
lemma_data <- read.csv(paste(resources_dir,'baseform_en.tsv', sep = "/"), encoding = "UTF-8")

# read an extended stop word list
stopwords_extended <- readLines(paste(resources_dir,'stopwords_en.txt', sep = "/"), encoding = "UTF-8")

# Preprocessing of the corpus of sentences
corpus_tokens <- corpus_sentences %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
  tokens_remove(pattern = stopwords_extended, padding = T)

sotu_collocations <- textstat_collocations(corpus_tokens, min_count = 25)
sotu_collocations <- sotu_collocations[1:250, ]

corpus_tokens <- tokens_compound(corpus_tokens, sotu_collocations)

# Create DTM, but remove terms which occur in less than 1% of all documents
DTM <- corpus_tokens %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = 0.01, max_docfreq = 1, docfreq_type = "prop")

# have a look at the number of documents and terms in the matrix
dim(DTM)

#trimming most common words fro improving accuracy -- they can be domain-relevant
top10_terms <- c( "unite_state", "past_year", "year_ago", "year_end", "government", "state", "country", "year", "make", "seek")

DTM <- DTM[, !(colnames(DTM) %in% top10_terms)]

# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- rowSums(DTM) > 0
DTM <- DTM[sel_idx, ]

# number of topics -- arbitrary 
K <- 20

# compute the LDA model, inference via n iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, seed = 1, verbose = 25))

# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)

#take a look at the 10 most likely terms within the term probabilities
terms(topicModel, 10)

#Giving the topics more descriptive names than just numbers.
# concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")
topicNames

# visualization and reporting [wordcloud]
# change for your own topic of interest
#topicToViz <- 1

# Or select a topic by a term contained in  its name
topicToViz <- grep('change', topicNames)[1] 

# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
wordcloud2(data.frame(words, probabilities), shuffle = FALSE, size = 0.8)


# visualization and reporting [topic ranking per document]
exampleIds <- c(1, 2, 3, 4, 5, 6)
N <- length(exampleIds)

# get topic proportions form example documents
# for every document we have a probability distribution of its contained topics
theta <- tmResult$topics 
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames 
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")  


ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)

```


``` {r coocuurance analysis}
# here we will follow a different text processing pipeline to implement the method discussed by: Andreas Niekler, Gregor Wiedemann -- The sequence and logic of processing remains constant across previous and this method, however, the syntax will differ slightly. Source: https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html

#re-importing the corpus without any transformation as we need it for sentence detection
extracted_texts <- readtext(Reports, docvarsfrom = "filepaths", dvsep = "/")

#text to corpus
sotu_corpus <- corpus(extracted_texts$text,
                      docnames = extracted_texts$doc_id)

#corpus to sentences
corpus_sentences <- corpus_reshape(sotu_corpus, to = "sentences")

#applying transformation

# Build a dictionary of lemmas
lemma_data <- read.csv(paste(resources_dir,'baseform_en.tsv', sep = "/"), encoding = "UTF-8")

# read an extended stop word list
stopwords_extended <- readLines(paste(resources_dir,'stopwords_en.txt', sep = "/"), encoding = "UTF-8")

# Preprocessing of the corpus of sentences
corpus_tokens <- corpus_sentences %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "fixed") %>%
  tokens_remove(pattern = stopwords_extended, padding = T)

#create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. In addition, we are interested in words that the do occur -- note that the interest is not how frequent they occur but that they occur together. 
minimumFrequency <- 10

# Create DTM, prune vocabulary and set binary values for presence/absence of types
binDTM <- corpus_tokens %>%
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = 1000000) %>%
  dfm_weight("boolean")

# Matrix multiplication for co-occurrence counts
coocCounts <- t(binDTM) %*% binDTM
as.matrix(coocCounts[202:205, 202:205])

# Read in the source code for the co-occurrence calculation to calculate Statistical significance -- in order to not only count joint occurrence we have to determine their significance this because only frequently co-occurring could result in poor indicator of meaning. 

source("calculateCoocStatistics.R")

# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 10
# Determination of the term of which co-competitors are to be measured.
coocTerm <- "woman"

coocs <- calculateCoocStatistics(coocTerm, binDTM, measure="LOGLIK")

# Display the numberOfCoocs main terms
print(coocs[1:numberOfCoocs])

```

``` {r visualizing cooccurances}
#The network graph would help with acquiring an extended semantic environment, by providing a secondary co -occurrence with the terms that co - occurred with the term "woman"

#For a term co-occurrence network, each triple consists of the target word, a co-occurring word and the significance of their joint occurrence. We denote the values with from, to, sig.

resultGraph <- data.frame(from = character(),
                          to = character(),
                          sig = numeric(0))

#First, we obtain all significant co-occurrence terms for the target term. Second, we obtain all co-occurrences of the co-occurrence terms from step one.

# The structure of the temporary graph object is equal to that of the resultGraph
tmpGraph <- data.frame(from = character(),
                       to = character(),
                       sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]

# Entry of the search word into the first column in all lines
tmpGraph[, 1] <- coocTerm

# Entry of the co-occurrences into the second column of the respective line
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]

# Set the significances
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

# Attach the triples to resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs) {

# Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure = "LOGLIK")
  
#print the co-occurrences
  coocs2[1:10]
  
# Structure of the temporary graph object
  tmpGraph <- data.frame(from = character(),
                         to = character(),
                         sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
#Append the result to the result graph
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

#Graphing

# set seed for graph plot
set.seed(1)

# Create the graph object as undirected graph
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identification of all nodes with less than 2 edges [co-occurring terms connected to at least 2 terms - filtering out terms cooccuring to only to one term.]
verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 2]
# These edges are removed from the graph
graphNetwork <- delete.vertices(graphNetwork, verticesToRemove)

# Assign colors to nodes (search term blue, others orange)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange')

# Set edge colors
E(graphNetwork)$color <- adjustcolor("DarkGray", alpha.f = .5)
# scale significance between 1 and 10 for edge width
E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))

# Set edges with radius
E(graphNetwork)$curved <- 0.15
# Size the nodes by their degree of networking (scaled between 5 and 15)
V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))

# Define the frame and spacing for the plot
par(mai = c(0, 0, 1, 0))

# Final Plot
plot(
  graphNetwork,
  layout = layout.fruchterman.reingold,
  # Force Directed Layout
  main = paste(coocTerm, ' Graph'),
  vertex.label.family = "sans",
  vertex.label.cex = 0.8,
  vertex.shape = "circle",
  vertex.label.dist = 0.5,
  # Labels of the nodes moved slightly
  vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
  vertex.label.color = 'black',
  # Color of node names
  vertex.label.font = 2,
  # Font of node names
  vertex.label = V(graphNetwork)$name,
  # node names
  vertex.label.cex = 1 # font size of node names
)


```

``` {r Extracting sentences with terms of interest "Woman" and "Women"}

#re-read the csv into data frame
#re-importing the corpus without any transformation as we need it for sentence detection
extracted_texts <- readtext(Reports, docvarsfrom = "filepaths", dvsep = "/")

#text to corpus
sotu_corpus <- corpus(extracted_texts$text,
                      docnames = extracted_texts$doc_id)


# Corpus to string [NLP deals with string]
doc<- as.String(sotu_corpus)

# Annotating sentences
sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator() 
sent_token_annotator
a1 <- NLP::annotate(doc, sent_token_annotator)
a1[1:5]

# converting into dataframe
df <- as.data.frame(as.matrix(doc[a1]))

#sub-setting for a pattern, parsing the text & extracting in csv file
terms<- grepl.sub(data = df, pattern = c("women", "woman"),Var = "V1")

print(terms)

write.csv(terms, paste(save_dir,'terms_in_extracts.csv'), fileEncoding = "UTF-8", row.names = FALSE)


```