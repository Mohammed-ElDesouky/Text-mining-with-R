"0","# here we will follow a different text processing pipeline to implement the method discussed by: Andreas Niekler, Gregor Wiedemann -- The sequence and logic of processing remains constant across prvious and this method, however, the syntax will differ slightly. "
"0",""
"0","#re-importing the corpus without any transformation as we need it for sentence detection"
"0","extracted_texts <- readtext(Reports, docvarsfrom = ""filepaths"", dvsep = ""/"")"
"0",""
"0","#save as csv for othe uses if needed"
"0","write.csv2(extracted_texts, paste(save_dir,'text_extracts.csv', sep = ""/""), fileEncoding = ""UTF-8"")"
"0",""
"0","#re-read the csv into data frame"
"0","text <- read.csv(paste(save_dir,'text_extracts.csv', sep = ""/""), header = T, sep = "";"", encoding = ""UTF-8"")     "
"0",""
"0","#text to corpus"
"0","sotu_corpus <- corpus(text$text,"
"0","                      docnames = text$doc_id)"
"0",""
"0","#corpus to sentences"
"0","corpus_sentences <- corpus_reshape(sotu_corpus, to = ""sentences"")"
"0",""
"0","#applying transformation"
"0",""
"0","# Build a dictionary of lemmas"
"0","lemma_data <- read.csv(paste(resources_dir,'baseform_en.tsv', sep = ""/""), encoding = ""UTF-8"")"
"0",""
"0","# read an extended stop word list"
"0","stopwords_extended <- readLines(paste(resources_dir,'stopwords_en.txt', sep = ""/""), encoding = ""UTF-8"")"
"0",""
"0","# Preprocessing of the corpus of sentences"
"0","corpus_tokens <- corpus_sentences %>%"
"0","  tokens("
"0","    remove_punct = TRUE,"
"0","    remove_numbers = TRUE,"
"0","    remove_symbols = TRUE"
"0","  ) %>%"
"0","  tokens_tolower() %>%"
"0","  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = ""fixed"") %>%"
"0","  tokens_remove(pattern = stopwords_extended, padding = T)"
"0",""
"0","#create a document-term-matrix. Only word forms which occur at least 10 times should be taken into account. In addition, we are interested in words that the do occur -- note that the interest is not how frequent they occur but that they occur together. "
"0","minimumFrequency <- 10"
"0",""
"0","# Create DTM, prune vocabulary and set binary values for presence/absence of types"
"0","binDTM <- corpus_tokens %>%"
"0","  tokens_remove("""") %>%"
"0","  dfm() %>%"
"0","  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = 1000000) %>%"
"0","  dfm_weight(""boolean"")"
"0",""
"0","# Matrix multiplication for co-occurrence counts"
"0","coocCounts <- t(binDTM) %*% binDTM"
"0","as.matrix(coocCounts[202:205, 202:205])"
"1","         "
"1"," stand"
"1"," credit"
"1"," corporate"
"1"," strength"
"1","
stand    "
"1","    11"
"1","      0"
"1","         0"
"1","        1"
"1","
credit   "
"1","     0"
"1","     40"
"1","         3"
"1","        1"
"1","
corporate"
"1","     0"
"1","      3"
"1","        19"
"1","        0"
"1","
strength "
"1","     1"
"1","      1"
"1","         0"
"1","       20"
"1","
"
"0","# Read in the source code for the co-occurrence calculation to calculate Statistical significance -- in order to not only count joint occurrence we have to determine their significance this because only frequently co-occurring could result in poor indicator of meaning. "
"0",""
"0","source(""calculateCoocStatistics.R"")"
"0",""
"0","# Definition of a parameter for the representation of the co-occurrences of a concept"
"0","numberOfCoocs <- 10"
"0","# Determination of the term of which co-competitors are to be measured."
"0","coocTerm <- ""woman"""
"0",""
"0","coocs <- calculateCoocStatistics(coocTerm, binDTM, measure=""LOGLIK"")"
"0",""
"0","# Display the numberOfCoocs main terms"
"0","print(coocs[1:numberOfCoocs])"
"1","     black "
"1","      move "
"1","initiative "
"1","     class "
"1","     focus "
"1","  employee "
"1","   advance "
"1","
"
"1"," 25.187276 "
"1"," 17.460013 "
"1","  8.829124 "
"1","  8.423543 "
"1","  8.012194 "
"1","  8.012194 "
"1","  5.820992 "
"1","
"
"1","    expand "
"1","    access "
"1","    invest "
"1","
"
"1","  5.820992 "
"1","  5.820992 "
"1","  5.474627 "
"1","
"
